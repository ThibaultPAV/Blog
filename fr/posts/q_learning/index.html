<!DOCTYPE html>
<html lang="fr"><head><meta charset="utf-8">
<meta http-equiv="content-type" content="text/html">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title itemprop="name">Q-learning | NeuroFab</title>
<meta property="og:title" content="Q-learning | NeuroFab" />
<meta name="twitter:title" content="Q-learning | NeuroFab" />
<meta itemprop="name" content="Q-learning | NeuroFab" />
<meta name="application-name" content="Q-learning | NeuroFab" />
<meta property="og:site_name" content="NeuroFab" />

<meta name="description" content="Un tema Hugo veloce e minimalista con supporto per la modalit√† chiara e scura, per la gestione di un sito o di un blog personale">
<meta itemprop="description" content="Un tema Hugo veloce e minimalista con supporto per la modalit√† chiara e scura, per la gestione di un sito o di un blog personale" />
<meta property="og:description" content="Un tema Hugo veloce e minimalista con supporto per la modalit√† chiara e scura, per la gestione di un sito o di un blog personale" />
<meta name="twitter:description" content="Un tema Hugo veloce e minimalista con supporto per la modalit√† chiara e scura, per la gestione di un sito o di un blog personale" />

<meta property="og:locale" content="fr" />
<meta name="language" content="fr" />

  <link rel="alternate" hreflang="en" href="https://thibaultpav.github.io/Blog/en/posts/q_learning/" title="English" />

  <link rel="alternate" hreflang="fr" href="https://thibaultpav.github.io/Blog/fr/posts/q_learning/" title="Fran√ßais" />





    
    
    

    <meta property="og:type" content="article" />
    <meta property="og:article:published_time" content=2024-08-01T00:00:00Z />
    <meta property="article:published_time" content=2024-08-01T00:00:00Z />
    <meta property="og:url" content="https://thibaultpav.github.io/Blog/fr/posts/q_learning/" />

    
    <meta property="og:article:author" content="Sidharth R" />
    <meta property="article:author" content="Sidharth R" />
    <meta name="author" content="Sidharth R" />
    
    

    

    <script defer type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "Article",
        "headline": "Q-learning",
        "author": {
        "@type": "Person",
        "name": ""
        },
        "datePublished": "2024-08-01",
        "description": "",
        "wordCount":  2410 ,
        "mainEntityOfPage": "True",
        "dateModified": "2024-08-01",
        "image": {
        "@type": "imageObject",
        "url": ""
        },
        "publisher": {
        "@type": "Organization",
        "name": "NeuroFab"
        }
    }
    </script>


<meta name="generator" content="Hugo 0.134.2">

    

    <link rel="canonical" href="https://thibaultpav.github.io/Blog/fr/posts/q_learning/">
    <link href="/Blog/style.min.d43bc6c79baa87f006efb2b92be952faeedeb1a3ab626c1d6abda52eae049355.css" rel="stylesheet">
    <link href="/Blog/code-highlight.min.706d31975fec544a864cb7f0d847a73ea55ca1df91bf495fd12a177138d807cf.css" rel="stylesheet">

    
    <link rel="apple-touch-icon" sizes="180x180" href="/Blog/icons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/Blog/icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/Blog/icons/favicon-16x16.png">
    <link rel="mask-icon" href="/Blog/icons/safari-pinned-tab.svg">
    <link rel="shortcut icon" href="/Blog/favicon.ico">




<link rel="manifest" href="https://thibaultpav.github.io/Blog/site.webmanifest">

<meta name="msapplication-config" content="/Blog/browserconfig.xml">
<meta name="msapplication-TileColor" content="#2d89ef">
<meta name="theme-color" content="#434648">

    
    <link rel="icon" type="image/svg+xml" href="/Blog/icons/favicon.svg">

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"
    integrity="sha512-fHwaWebuwA7NSF5Qg/af4UeDx9XqUpYpOGgubo3yWu+b2IQR4UeQwbb42Ti7gVAjNtVoI/I9TEoYeu9omwcC6g==" crossorigin="anonymous" crossorigin="anonymous" />


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"
    integrity="sha512-LQNxIMR5rXv7o+b1l8+N1EZMfhG7iFZ9HhnbJkTp4zjNr5Wvst75AqUeFDxeRUa7l5vEDyUiAip//r+EFLLCyA=="
    crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    integrity="sha512-iWiuBS5nt6r60fCz26Nd0Zqe0nbk1ZTIQbl3Kv7kYsX+yKMUFHzjaH2+AnM6vp2Xs+gNmaBAVWJjSmuPw76Efg==" crossorigin="anonymous" onload="renderMathInElement(document.body, {
      delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '$', right: '$', display: false}
      ]
    });"></script>

    </head>
<body data-theme = "dark" class="notransition">

<script src="/Blog/js/theme.min.8961c317c5b88b953fe27525839672c9343f1058ab044696ca225656c8ba2ab0.js" integrity="sha256-iWHDF8W4i5U/4nUlg5ZyyTQ/EFirBEaWyiJWVsi6KrA="></script>

<div class="navbar" role="navigation">
    <nav class="menu" aria-label="Main Navigation">
        <a href="https://thibaultpav.github.io/Blog/fr/" class="logo">
            <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" 
viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" 
stroke-linejoin="round" class="feather feather-home">
<title></title>
<path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path>
<polyline points="9 22 9 12 15 12 15 22"></polyline>
</svg>
        </a>
        <input type="checkbox" id="menu-trigger" class="menu-trigger" />
        <label for="menu-trigger">
            <span class="menu-icon">
                <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" stroke="currentColor" fill="none" viewBox="0 0 14 14"><title>Menu</title><path stroke-linecap="round" stroke-linejoin="round" d="M10.595 7L3.40726 7"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 3.51488L3.49301 3.51488"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 10.4851H3.49301"></path><path stroke-linecap="round" stroke-linejoin="round" d="M0.5 12.5V1.5C0.5 0.947715 0.947715 0.5 1.5 0.5H12.5C13.0523 0.5 13.5 0.947715 13.5 1.5V12.5C13.5 13.0523 13.0523 13.5 12.5 13.5H1.5C0.947715 13.5 0.5 13.0523 0.5 12.5Z"></path></svg>
            </span>
        </label>

        <div class="trigger">
            <ul class="trigger-container">
                
                
                <li>
                    <a class="menu-link " href="/Blog/fr/">
                        Accueil
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link active" href="/Blog/fr/posts/">
                        Articles
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link " href="/Blog/fr/pages/about/">
                        √Ä Propos
                    </a>
                    
                </li>
                
                <li class="menu-separator">
                    <span>|</span>
                </li>
                
                
                    <li>
                        <select aria-label="Select Language" class="lang-list" id="select-language" onchange="location = this.value;">
                            
                            
                            
                                
                                
                                    
                                        
                                        
                                            <option id="en" value="https://thibaultpav.github.io/Blog/en/posts/q_learning/">EN</option>
                                        
                                    
                                
                                    
                                
                            
                                
                                
                                    
                                
                                    
                                        
                                        
                                            <option id="fr" value="https://thibaultpav.github.io/Blog/fr/posts/q_learning/" selected>FR
                                            </option>
                                        
                                    
                                
                            
                        </select>                
                    </li>
                    <li class="menu-separator">
                        <span>|</span>
                    </li>
                
            </ul>
            <a id="mode" href="#">
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-sunny" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>LIGHT</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-moon" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>DARK</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
            </a>
        </div>
    </nav>
</div>

<div class="wrapper post">
    <main class="page-content" aria-label="Content">
        <article>
            <header class="header">
                <h1 class="header-title">Q-learning</h1>
                
                
                <div class="post-meta">
                    <time datetime="2024-08-01T00:00:00&#43;00:00" itemprop="datePublished"> 1 ao√ªt 2024 </time>
                </div>
                
            </header>
            
    
    <details class="toc" ZgotmplZ>
        <summary><b></b></summary>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#principes-de-base-et-fonctionnement-du-q-learning">Principes de Base et fonctionnement du Q-learning</a></li>
    <li><a href="#algorithme-du-q-learning">Algorithme du Q-learning</a>
      <ul>
        <li><a href="#1-initialisation"><strong>1. Initialisation</strong></a></li>
        <li><a href="#2-boucle-principale-dapprentissage"><strong>2. Boucle Principale d&rsquo;Apprentissage</strong></a></li>
        <li><a href="#3-fin-de-l√©pisode-et-convergence"><strong>3. Fin de l&rsquo;√âpisode et Convergence</strong></a></li>
        <li><a href="#strat√©gies-dexploration-et-convergence"><strong>Strat√©gies d&rsquo;Exploration et Convergence</strong></a></li>
      </ul>
    </li>
    <li><a href="#application-du-q-learning-√†-frozen-lake">Application du Q-learning √† Frozen Lake</a>
      <ul>
        <li><a href="#pr√©sentation-de-frozen-lake"><strong>Pr√©sentation de Frozen Lake</strong></a></li>
        <li><a href="#impl√©mentation-avec-q-learning"><strong>Impl√©mentation avec Q-learning</strong></a>
          <ul>
            <li><a href="#1-classe-environment"><strong>1. Classe Environment</strong></a></li>
            <li><a href="#2-classe-param√®tre"><strong>2. Classe Param√®tre</strong></a></li>
            <li><a href="#3-classe-agent"><strong>3. Classe Agent</strong></a></li>
            <li><a href="#4-entra√Ænement-de-lagent"><strong>4. Entra√Ænement de l&rsquo;Agent</strong></a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#analyse-des-r√©sultats">Analyse des R√©sultats</a>
      <ul>
        <li><a href="#heatmap-des-trajectoires-privil√©gi√©es"><strong>Heatmap des Trajectoires Privil√©gi√©es</strong></a></li>
        <li><a href="#nombre-de-pas-par-episode"><strong>Nombre de Pas par Episode</strong></a></li>
        <li><a href="#r√©compense-cumul√©e-par-episode"><strong>R√©compense Cumul√©e par Episode</strong></a></li>
      </ul>
    </li>
  </ul>
</nav>
    </details>
            <div class="page-content">
                <p>Maintenant que nous avons compris les bases des m√©thodes de diff√©rence temporelle (<a href="/Blog/fr/posts/introduction_rl/">introduction_Rl</a>), passons √† l&rsquo;une des m√©thodes les plus connues en apprentissage par renforcement : le Q-learning. üòÉ</p>
<p>Pour bien comprendre son fonctionnement, nous allons d&rsquo;abord introduire l&rsquo;algorithme du Q-learning avant de le mettre en pratique dans un environnement de jeu vid√©o appel√© Frozen Lake. Les codes et exemples que nous utiliserons seront disponibles sur GitHub pour que vous puissiez les consulter et les essayer par vous-m√™me.</p>
<h2 id="principes-de-base-et-fonctionnement-du-q-learning">Principes de Base et fonctionnement du Q-learning</h2>
<p>Le Q-learning est une m√©thode d&rsquo;apprentissage par renforcement qui permet √† un agent d&rsquo;apprendre √† prendre des d√©cisions optimales en interagissant avec son environnement, en estimant la qualit√© des actions dans des √©tats donn√©s. Contrairement √† d&rsquo;autres m√©thodes d&rsquo;apprentissage supervis√©, le Q-learning ne n√©cessite pas de donn√©es pr√©-√©tiquet√©es. Au lieu de cela, l&rsquo;agent d√©couvre la valeur de diff√©rentes actions en les testant, puis ajuste ses d√©cisions en fonction des r√©sultats obtenus.</p>
<p>Au c≈ìur du Q-learning se trouve la fonction de valeur Q. Cette fonction estime la qualit√© d&rsquo;une action dans un √©tat donn√©, en termes de r√©compense cumulative attendue. üöÄ</p>
<p>En d&rsquo;autres termes, l&rsquo;algorithme met √† jour la valeur de chaque action en tenant compte non seulement de la r√©compense imm√©diate obtenue, mais aussi de la valeur potentielle des actions futures. Cette approche permet √† l&rsquo;agent de d√©velopper une strat√©gie qui non seulement maximise les r√©compenses imm√©diates, mais aussi optimise les gains √† long terme.</p>
<p>Le processus d&rsquo;apprentissage dans le Q-learning suit les √©tapes suivantes :</p>
<ul>
<li><strong>Initialisation</strong> : L&rsquo;agent d√©marre avec une fonction de valeur Q initialis√©e √† des valeurs arbitraires pour commencer le processus d&rsquo;apprentissage.</li>
<li><strong>S√©lection de l&rsquo;Action</strong> : √Ä chaque √©tape, l&rsquo;agent choisit une action en fonction de la fonction Q actuelle. Une strat√©gie courante est l&rsquo;Œµ-greedy, o√π l&rsquo;agent choisit l&rsquo;action avec la meilleure valeur Q la plupart du temps, mais explore de nouvelles actions avec une probabilit√© Œµ.</li>
<li><strong>Ex√©cution de l&rsquo;Action et Observation</strong> : L&rsquo;agent ex√©cute l&rsquo;action, re√ßoit une r√©compense, et observe le nouvel √©tat de l&rsquo;environnement.</li>
<li><strong>Mise √† Jour de la Valeur Q</strong> : L&rsquo;agent met √† jour la valeur Q de l&rsquo;√©tat-action pr√©c√©dent en utilisant l&rsquo;√©quation du Q-learning, prenant en compte la r√©compense imm√©diate et la valeur maximale des actions futures.</li>
<li><strong>R√©p√©tition</strong> : Le processus se r√©p√®te pour un nombre d√©fini d&rsquo;it√©rations, permettant √† l&rsquo;agent d&rsquo;am√©liorer progressivement sa strat√©gie. üòä</li>
</ul>
<p>Gr√¢ce √† la mise √† jour it√©rative de la fonction Q, le Q-learning tend √† converger vers la politique optimale apr√®s un certain nombre de r√©visions. Cela signifie que, quel que soit l&rsquo;√©tat initial, l&rsquo;agent apprend √† prendre les d√©cisions qui maximisent la r√©compense √† long terme.</p>
<h2 id="algorithme-du-q-learning">Algorithme du Q-learning</h2>
<p>Voici une description d√©taill√©e de l&rsquo;algorithme :</p>
<h3 id="1-initialisation"><strong>1. Initialisation</strong></h3>
<p>L&rsquo;algorithme commence par initialiser une table de valeurs Q, appel√©e <strong>Q-table</strong>. Cette table associe chaque paire √©tat-action √† une valeur de Q, qui repr√©sente la qualit√© de l&rsquo;action pour cet √©tat donn√©. Au d√©but, ces valeurs peuvent √™tre initialis√©es √† z√©ro ou √† des valeurs al√©atoires. L&rsquo;agent n&rsquo;a aucune connaissance pr√©alable de l&rsquo;environnement et doit donc d√©couvrir la meilleure politique par l&rsquo;exploration.</p>
<h3 id="2-boucle-principale-dapprentissage"><strong>2. Boucle Principale d&rsquo;Apprentissage</strong></h3>
<p>L&rsquo;algorithme se d√©roule sur un certain nombre d&rsquo;<strong>√©pisodes</strong>, o√π chaque √©pisode repr√©sente une s√©quence compl√®te d&rsquo;interactions entre l&rsquo;agent et l&rsquo;environnement, depuis un √©tat initial jusqu&rsquo;√† un √©tat terminal.</p>
<p>Pour chaque √©pisode :</p>
<ol>
<li>
<p><strong>S√©lection de l&rsquo;√âtat Initial</strong> : L&rsquo;agent commence dans un √©tat initial choisi soit de mani√®re al√©atoire, soit selon des r√®gles sp√©cifiques.</p>
</li>
<li>
<p><strong>Boucle d&rsquo;Actions</strong> :</p>
<ul>
<li>
<p><strong>S√©lection de l&rsquo;Action</strong> : √Ä chaque √©tape, l&rsquo;agent doit choisir une action √† partir de l&rsquo;√©tat actuel. Cette d√©cision est guid√©e par une strat√©gie, souvent la strat√©gie <strong>Œµ-greedy</strong> :</p>
<ul>
<li>Avec une probabilit√© Œµ, l&rsquo;agent choisit une action al√©atoire (exploration). ü§î</li>
<li>Avec une probabilit√© 1-Œµ, l&rsquo;agent choisit l&rsquo;action ayant la valeur Q la plus √©lev√©e pour l&rsquo;√©tat actuel (exploitation).</li>
</ul>
</li>
<li>
<p><strong>Ex√©cution de l&rsquo;Action</strong> : L&rsquo;agent ex√©cute l&rsquo;action choisie et observe la r√©compense obtenue ainsi que l&rsquo;√©tat suivant dans lequel il se retrouve.</p>
</li>
<li>
<p><strong>Mise √† Jour de la Valeur Q</strong> : La valeur Q de la paire √©tat-action pr√©c√©dente est mise √† jour en utilisant l&rsquo;√©quation suivante :</p>
</li>
</ul>
<p>$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \cdot \max_{a&rsquo;} Q(s&rsquo;, a&rsquo;) - Q(s, a) \right]
$$</p>
</li>
</ol>
<ul>
<li>
<p>O√π :</p>
<ul>
<li>$s$  est l&rsquo;√©tat actuel.</li>
<li>$a$  est l&rsquo;action choisie.</li>
<li>$r$  est la r√©compense obtenue apr√®s avoir pris l&rsquo;action $a$.</li>
<li>$s&rsquo;$  est l&rsquo;√©tat suivant apr√®s l&rsquo;action $a$.</li>
<li>$\alpha $ est le taux d&rsquo;apprentissage (learning rate), qui d√©termine √† quel point les nouvelles informations mises √† jour dans Q doivent remplacer l&rsquo;ancienne information.</li>
<li>$ \gamma $ est le facteur d&rsquo;actualisation (discount factor), qui d√©termine l&rsquo;importance des r√©compenses futures par rapport aux r√©compenses imm√©diates.</li>
<li>$ \max_{a&rsquo;} Q(s&rsquo;, a&rsquo;) $ repr√©sente la valeur Q maximale pour le meilleur choix d&rsquo;action possible dans l&rsquo;√©tat $s&rsquo;$ √† la prochaine √©tape.</li>
</ul>
</li>
<li>
<p><strong>Transition vers le Nouvel √âtat</strong> : L&rsquo;agent passe alors au nouvel √©tat ( s&rsquo; ) et r√©p√®te le processus jusqu&rsquo;√† ce qu&rsquo;il atteigne un √©tat terminal (fin de l&rsquo;√©pisode).</p>
</li>
</ul>
<h3 id="3-fin-de-l√©pisode-et-convergence"><strong>3. Fin de l&rsquo;√âpisode et Convergence</strong></h3>
<p>Apr√®s un certain nombre d&rsquo;it√©rations (√©pisodes), les valeurs Q commencent √† converger vers des valeurs stables, refl√©tant la politique optimale. L&rsquo;agent aura appris quelle action prendre dans chaque √©tat pour maximiser les r√©compenses sur le long terme. üëç</p>
<h3 id="strat√©gies-dexploration-et-convergence"><strong>Strat√©gies d&rsquo;Exploration et Convergence</strong></h3>
<p>L&rsquo;algorithme de Q-learning repose sur un √©quilibre entre exploration (essayer de nouvelles actions pour d√©couvrir leur valeur) et exploitation (utiliser les connaissances actuelles pour maximiser la r√©compense). L&rsquo;utilisation de la strat√©gie Œµ-greedy est courante, mais il existe d&rsquo;autres strat√©gies, comme l&rsquo;<strong>exploration par d√©clin de taux Œµ</strong>, o√π la probabilit√© Œµ diminue au fil du temps, permettant une exploration intensive au d√©but puis une exploitation accrue √† mesure que l&rsquo;agent apprend.</p>
<p>En fin de compte, le Q-learning converge g√©n√©ralement vers une politique optimale, √† condition que tous les √©tats et actions soient suffisamment explor√©s, et que les valeurs Q soient correctement mises √† jour.</p>
<h2 id="application-du-q-learning-√†-frozen-lake">Application du Q-learning √† Frozen Lake</h2>
<p>Pour illustrer le fonctionnement du Q-learning, nous allons l&rsquo;appliquer √† un environnement de jeu simple appel√© <em>Frozen Lake</em>. Cet environnement est couramment utilis√© pour d√©montrer les concepts de l&rsquo;apprentissage par renforcement.</p>
<h3 id="pr√©sentation-de-frozen-lake"><strong>Pr√©sentation de Frozen Lake</strong></h3>
<p><em>Frozen Lake</em> est un jeu de plateau o√π un agent (repr√©sent√© par un personnage) doit traverser un lac gel√© pour atteindre un but, tout en √©vitant de tomber dans les trous cach√©s sous la glace. Le lac est repr√©sent√© par une grille, o√π chaque case peut √™tre :</p>
<ul>
<li><strong>S</strong> : la position de d√©part (Start).</li>
<li><strong>F</strong> : une case de glace (Frozen).</li>
<li><strong>H</strong> : un trou dans la glace (Hole), qui termine imm√©diatement le jeu si l&rsquo;agent y tombe.</li>
<li><strong>G</strong> : l&rsquo;objectif (Goal), que l&rsquo;agent doit atteindre pour gagner.</li>
</ul>
<p>L&rsquo;agent peut se d√©placer dans quatre directions : haut, bas, gauche et droite. Le but est d&rsquo;atteindre la case <em>G</em> en suivant le chemin le plus s√ªr, en √©vitant les trous. Cependant, le v√©ritable d√©fi r√©side dans la nature glissante de la glace. En effet, la glace est tra√Ætresse : m√™me si l&rsquo;agent choisit une direction, il n&rsquo;est pas garanti qu&rsquo;il s&rsquo;arr√™te imm√©diatement sur la case suivante. Au lieu de cela, l&rsquo;agent peut continuer √† glisser dans la direction choisie, ce qui rend ses mouvements impr√©visibles.</p>
<p>Ce ph√©nom√®ne de glissade ajoute un aspect stochastique au jeu, o√π les actions de l&rsquo;agent ne produisent pas toujours les r√©sultats escompt√©s. Cela signifie que m√™me avec une strat√©gie optimale, l&rsquo;agent doit faire face √† une certaine incertitude quant √† l&rsquo;endroit exact o√π il atterrira apr√®s un mouvement. Par cons√©quent, l&rsquo;agent doit non seulement planifier ses actions en fonction des r√©compenses et des risques imm√©diats, mais aussi tenir compte de la probabilit√© que sa trajectoire d√©vie en raison de la glace glissante. üò¨
Pour plus de simplicit√©, dans cet article nous nous concentrerons sur un environnement non glissant.</p>
<p><img alt="Jeu Frozen Lake" src="/Blog/fr/posts/q_learning/jeu.PNG"></p>
<h3 id="impl√©mentation-avec-q-learning"><strong>Impl√©mentation avec Q-learning</strong></h3>
<p>Pour impl√©menter le Q-learning dans cet environnement, nous avons structur√© le code en deux classes principales : <strong>Agent</strong> et <strong>Environment</strong>. L&rsquo;environnement <em>Frozen Lake</em> a √©t√© r√©cup√©r√© √† l&rsquo;aide de la biblioth√®que <em>Gym</em>, qui est largement utilis√©e pour tester les algorithmes d&rsquo;apprentissage par renforcement.</p>
<h4 id="1-classe-environment"><strong>1. Classe Environment</strong></h4>
<p>La classe <strong>Environment</strong> encapsule les aspects de l&rsquo;environnement de <em>Frozen Lake</em> fourni par <em>Gym</em>. Elle g√®re les interactions entre l&rsquo;agent et l&rsquo;environnement, telles que :</p>
<ul>
<li>La r√©initialisation de l&rsquo;environnement au d√©but de chaque √©pisode.</li>
<li>La gestion des actions de l&rsquo;agent (d√©placements).</li>
<li>L&rsquo;observation des nouveaux √©tats apr√®s chaque action.</li>
<li>Le calcul des r√©compenses en fonction de l&rsquo;√©tat actuel de l&rsquo;agent.</li>
</ul>
<h4 id="2-classe-param√®tre"><strong>2. Classe Param√®tre</strong></h4>
<p>La classe <strong>Param√®tre</strong> est un √©l√©ment central de l&rsquo;entra√Ænement de l&rsquo;agent, regroupant tous les hyperparam√®tres essentiels qui influencent non seulement la vitesse de convergence, mais aussi la performance finale du mod√®le.</p>
<ul>
<li><strong>lr (Learning Rate)</strong> : Le taux d&rsquo;apprentissage d√©termine √† quelle vitesse l&rsquo;agent met √† jour ses connaissances.</li>
<li><strong>gamma (Discount Factor)</strong> : Le facteur de discount influence l&rsquo;importance accord√©e aux r√©compenses futures. Un gamma √©lev√© incite l&rsquo;agent √† planifier sur le long terme, tandis qu&rsquo;un gamma plus faible privil√©gie les gains imm√©diats.</li>
<li><strong>eps (Exploration Rate)</strong> : Le taux d&rsquo;exploration initial contr√¥le la la tendance de l&rsquo;agent √† explorer de nouvelles actions.</li>
<li><strong>eps_d (Decay Rate of Exploration)</strong> : Le facteur de d√©croissance r√©duit progressivement l&rsquo;exploration au fil du temps, favorisant l&rsquo;exploitation des connaissances acquises.</li>
<li><strong>eps_min (Minimum Exploration Rate)</strong> : Ce param√®tre garantit que l&rsquo;agent continue √† explorer de nouvelles strat√©gies m√™me en fin d&rsquo;entra√Ænement, √©vitant ainsi le sur-apprentissage. üéØ</li>
<li><strong>nb_ep (Number of Episodes)</strong> : Le nombre d&rsquo;√©pisodes d&rsquo;entra√Ænement est crucial pour permettre √† l&rsquo;agent d&rsquo;explorer suffisamment l&rsquo;environnement et d&rsquo;affiner sa politique.</li>
<li><strong>is_slippery</strong> : Ce param√®tre d√©termine si la surface du lac est glissante ou non, ajoutant ainsi une complexit√© suppl√©mentaire au probl√®me, ce qui peut n√©cessiter un ajustement des autres hyperparam√®tres.</li>
<li><strong>map_name</strong> : Ce param√®tre sp√©cifie la taille du plateau (<em>par exemple</em> &ldquo;8x8&rdquo;), ce qui influence directement la complexit√© de l&rsquo;environnement que l&rsquo;agent doit ma√Ætriser.</li>
<li><strong>nb_run (Number of Runs)</strong> : Le nombre de runs permet de stabiliser les r√©sultats en r√©p√©tant l&rsquo;entra√Ænement plusieurs fois. Cela aide √† lisser les fluctuations al√©atoires et √† obtenir une Q-table plus fiable.</li>
</ul>
<h4 id="3-classe-agent"><strong>3. Classe Agent</strong></h4>
<p>La classe <strong>Agent</strong> impl√©mente l&rsquo;algorithme de Q-learning. Elle g√®re l&rsquo;apprentissage de l&rsquo;agent en interagissant avec l&rsquo;environnement, en prenant des d√©cisions, en mettant √† jour la Q-table, et en appliquant les strat√©gies d&rsquo;exploration et d&rsquo;exploitation. Les principales m√©thodes pr√©sentent dans cette classe permettent :</p>
<ul>
<li>La gestion de la <strong>Q-table</strong>, qui stocke les valeurs Q pour chaque paire √©tat-action.</li>
<li>La s√©lection des actions √† chaque √©tape en utilisant la strat√©gie <strong>Œµ-greedy</strong>.</li>
<li>La mise √† jour de la Q-table apr√®s chaque action, en fonction des r√©compenses obtenues et des √©tats suivants.</li>
<li>L&rsquo;apprentissage √† travers plusieurs √©pisodes pour converger vers la politique optimale.</li>
</ul>
<p>Voici un aper√ßu de la classe <strong>Agent</strong> :</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">QLearningAgent</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_state</span><span class="p">,</span> <span class="n">n_action</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span><span class="n">eps_d</span><span class="p">,</span><span class="n">eps_min</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">n_action</span> <span class="o">=</span> <span class="n">n_action</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">n_state</span> <span class="o">=</span> <span class="n">n_state</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_state</span><span class="p">,</span> <span class="n">n_action</span><span class="p">))</span> 
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">eps_d</span> <span class="o">=</span> <span class="n">eps_d</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">eps_min</span> <span class="o">=</span> <span class="n">eps_min</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">choose_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Choisit une action en utilisant la politique epsilon-greedy.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_action</span><span class="p">)</span>  <span class="c1"># Exploration</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:])</span>  <span class="c1"># Exploitation</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">action</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Met √† jour la Q-table en utilisant l&#39;√©quation de Q-learning.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">best_next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">next_state</span><span class="p">,</span> <span class="p">:])</span>
</span></span><span class="line"><span class="cl">        <span class="n">td_target</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">best_next_action</span>
</span></span><span class="line"><span class="cl">        <span class="n">td_error</span> <span class="o">=</span> <span class="n">td_target</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">td_error</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">update_exploration_rate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Met √† jour le taux d&#39;exploration (epsilon) pour favoriser l&#39;exploitation au fil du temps.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eps_min</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps_d</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">exploit_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Choisit la meilleure action selon la Q-table (sans exploration).
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:])</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">update_qtable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">qtable</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Met √† jour la Q-table en fonction de la Q-table donn√©e en parametre (sert √† moyenner les diff√©rents runs)
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="o">=</span><span class="n">qtable</span>
</span></span></code></pre></div><h4 id="4-entra√Ænement-de-lagent"><strong>4. Entra√Ænement de l&rsquo;Agent</strong></h4>
<p>L&rsquo;entra√Ænement de l&rsquo;agent consiste √† le faire interagir avec l&rsquo;environnement √† travers un grand nombre d&rsquo;√©pisodes. √Ä chaque √©pisode, l&rsquo;agent explore les diff√©rentes options et am√©liore progressivement sa Q-table, jusqu&rsquo;√† ce qu&rsquo;il trouve une strat√©gie stable et optimale pour traverser le lac sans tomber dans les trous.</p>
<p>L&rsquo;agent utilise la classe <strong>Environment</strong> pour interagir avec <em>Frozen Lake</em> et applique l&rsquo;algorithme de Q-learning pour apprendre la politique optimale. Le processus d&rsquo;apprentissage se d√©roule comme suit :</p>
<ul>
<li>L&rsquo;agent commence dans un √©tat initial al√©atoire.</li>
<li>Il choisit une action bas√©e sur sa strat√©gie (Œµ-greedy).</li>
<li>Il observe la r√©compense et l&rsquo;√©tat suivant r√©sultant de cette action.</li>
<li>Il met √† jour la Q-table en fonction de la r√©compense et des nouvelles informations obtenues.</li>
<li>Ce processus se r√©p√®te jusqu&rsquo;√† ce que l&rsquo;agent atteigne un √©tat terminal ou que l&rsquo;√©pisode se termine.</li>
</ul>
<p>Au fur et √† mesure que l&rsquo;agent s&rsquo;entra√Æne, il devient de plus en plus comp√©tent, trouvant le chemin optimal pour atteindre l&rsquo;objectif en √©vitant les trous.</p>
<p>Pour am√©liorer encore les performances et la stabilit√© de la politique apprise, l&rsquo;entra√Ænement est souvent r√©alis√© sur plusieurs it√©rations appel√©es &ldquo;runs&rdquo;. Chaque it√©ration correspond √† une session d&rsquo;entra√Ænement distincte o√π l&rsquo;agent repart de z√©ro, ce qui permet de r√©duire l&rsquo;aspect stochastique propre √† chaque Q-table obtenue lors des diff√©rentes it√©rations. Apr√®s avoir effectu√© toutes les sessions d&rsquo;entra√Ænement, les Q-tables obtenues sont moyenn√©es. Cela permet de lisser les √©ventuelles fluctuations al√©atoires et d&rsquo;obtenir une Q-table finale plus stable et fiable. üí™</p>
<h2 id="analyse-des-r√©sultats">Analyse des R√©sultats</h2>
<p>Apr√®s l&rsquo;entra√Ænement, il est crucial de comprendre comment l&rsquo;agent a perform√© et quelles strat√©gies il a adopt√©es. Voici une synth√®se des r√©sultats √† travers trois visualisations cl√©s.</p>
<h3 id="heatmap-des-trajectoires-privil√©gi√©es"><strong>Heatmap des Trajectoires Privil√©gi√©es</strong></h3>
<p>La <strong>heatmap</strong> montre les chemins pr√©f√©r√©s de l&rsquo;agent sur le plateau. Les zones o√π la couleur est plus intense r√©v√®lent les trajets que l&rsquo;agent consid√®re comme les plus s√ªrs et efficaces pour atteindre l&rsquo;objectif, en √©vitant les trous.</p>
<p><img alt="Heatmap des Trajectoires Privil√©gi√©es" src="/Blog/fr/posts/q_learning/jeu2.PNG"></p>
<h3 id="nombre-de-pas-par-episode"><strong>Nombre de Pas par Episode</strong></h3>
<p>Le graphique du <strong>nombre de pas par √©pisode</strong> illustre comment l&rsquo;efficacit√© de l&rsquo;agent s&rsquo;am√©liore avec le temps. Au d√©but, il prend plus de pas pour atteindre l&rsquo;objectif, mais ce nombre diminue √† mesure qu&rsquo;il apprend √† optimiser ses mouvements. √Ä la fin de l&rsquo;entra√Ænement, le nombre de pas se stabilise, montrant que l&rsquo;agent a trouv√© une strat√©gie quasi optimale. üòâ</p>
<h3 id="r√©compense-cumul√©e-par-episode"><strong>R√©compense Cumul√©e par Episode</strong></h3>
<p>Le graphique de la <strong>r√©compense cumul√©e</strong> met en √©vidence la progression de l&rsquo;agent. Au d√©part, les r√©compenses sont faibles, mais elles augmentent progressivement au fur et √† mesure que l&rsquo;agent affine sa strat√©gie. Une fois que la courbe se stabilise, cela indique que l&rsquo;agent a converg√© vers une politique stable et efficace, maximisant ainsi ses gains tout en minimisant les risques.</p>
<p><img alt="Nombre de Pas et R√©compense Cumul√©e par Episode" src="/Blog/fr/posts/q_learning/jeu3.png"></p>
<p>En r√©sum√©, ces visualisations d√©montrent que l&rsquo;agent a appris √† naviguer efficacement dans l&rsquo;environnement, optimisant son parcours pour maximiser les r√©compenses et atteindre son objectif de mani√®re fiable.</p>

            </div>
        </article></main>
</div>
<footer class="footer">
    <span class="footer_item"> </span>
    &nbsp;

    <div class="footer_social-icons">
<a href="https://github.com/ThibaultPAV" target="_blank" rel="noopener noreferrer me"
    title="Github">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path
        d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
    </path>
</svg>
</a>
<a href="https://www.linkedin.com/in/thibault-pawlisz-b2084a262/" target="_blank" rel="noopener noreferrer me"
    title="Linkedin">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path>
    <rect x="2" y="9" width="4" height="12"></rect>
    <circle cx="4" cy="4" r="2"></circle>
</svg>
</a>
<a href="mailto:pawlisz.thibault@gmail.com" target="_blank" rel="noopener noreferrer me"
    title="Email">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 21" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path>
    <polyline points="22,6 12,13 2,6"></polyline>
</svg>
</a>
</div>
    <small class="footer_copyright">
        ¬© 2024 Sidharth R.
        
    </small>
</footer><a href="#" title="" id="totop">
    <svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" fill="currentColor" stroke="currentColor" viewBox="0 96 960 960">
    <path d="M283 704.739 234.261 656 480 410.261 725.739 656 677 704.739l-197-197-197 197Z"/>
</svg>

</a>


    




    
    
        
    

    
    
        
    



    
    <script src="https://thibaultpav.github.io/Blog/js/main.min.35f435a5d8eac613c52daa28d8af544a4512337d3e95236e4a4978417b8dcb2f.js" integrity="sha256-NfQ1pdjqxhPFLaoo2K9USkUSM30&#43;lSNuSkl4QXuNyy8="></script>

    

</body>
</html>
